{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees and Machine Learning\n",
    "<li>Decision trees are tree structures containing rules\n",
    "<li>The leaf nodes of the tree are the \"learned\" categories (or threshold values)\n",
    "<li>A path from the root to a leaf node represents a rule\n",
    "<h3>Example: A decision tree with rules on deciding who survived or died on the titanic</h3>\n",
    "<i>Source: https://en.wikipedia.org/wiki/Decision_tree_learning#/media/File:CART_tree_titanic_survivors.png</i>\n",
    "<li>To use the tree, enter a person-data object and you'll get an answer</li>\n",
    "<li>Ex: (\"John Brown\",\"Male\",\"30 years old\", \"3 siblings\") Ans: Survived (89% probability)\n",
    "<li>Ex: (\"Jill Jones\", \"Female\",\"7 years old\", \"no siblings\") Ans: Survived (73% probability)\n",
    "<li>Ex: (\"Hercules Mulligan\", \"Male\", \"2 years old\", \"20 siblings\") Ans: Died (17% probability)\n",
    "<li>Note that the 17% probability doesn't mean that there is an 83% chance that Mulligan survived!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWgAAAFUCAMAAAA+g1YxAAAACXBIWXMAAAsTAAALEwEAmpwYAAAA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\naDoBgAAaDWg6AYAAGg1oOgGAAAMAmflvOK9Yn5AAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {
      "image/png": {
       "height": 400,
       "width": 400
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename = \"CART_tree_titanic_survivors.png\", width=400, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we look at this, the is sex male and the no side of it that\n",
    "is for females, then we see that there's\n",
    "a 0.73% probability that the person survived the Titanic\n",
    "whatever, sinking of the Titanic.\n",
    "And we see that 36% of the total cases in our data set\n",
    "are represented by this, so this is a pretty high confidence.\n",
    "36% is a lot of cases.\n",
    "\n",
    "Whereas, if you travel all the way down to these two,\n",
    "died and survived at the bottom of siblings greater than 2.5,\n",
    "the number of cases are just 2%.\n",
    "And that's a kind of a low number of cases.\n",
    "So what we're saying is that with a very small number\n",
    "of cases, we're trying to make a guess as to whether they\n",
    "survived or not.\n",
    "And that's not so great."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2>Types of decision trees</h2>\n",
    "Classification trees are used when there's a feature value,\n",
    "like a rock or a mine, and regression trees when there are\n",
    "continuous variables,\n",
    "<ul>\n",
    "<li><b>Classification trees</b>: Uses rules to classify cases into two or more categories (Rocks vs Mines)\n",
    "<ul>\n",
    "<li>Classification trees recursively split the data on a feature value\n",
    "<li>Each split minimizes the cost (also known as the impurity)\n",
    "<li>Cost is commonly measured using the GINI cost function (a measure of the probability of misclassification or 'purity')\n",
    "</ul>\n",
    "<li><b>Regression trees</b>: Uses rules to group data into target variable ranges (Wine Quality)\n",
    "\n",
    "Typically, in regression trees, what you want to do\n",
    "is you want to find groups like this, the _female survived_, that\n",
    "are represented by a __large number__ of cases\n",
    "so that you have a little more confidence in it.\n",
    "And then that is probably a better bet.\n",
    "\n",
    "<ul>\n",
    "<li>Also split the data on feature values\n",
    "<li>Minimize cost (impurity). Usually the mean squared error\n",
    "</ul>\n",
    "</ul>\n",
    "And the whole idea in the tree is\n",
    "to somehow find these splitting points, like should\n",
    "be split on male/female first?\n",
    "Should we split on age first?\n",
    "Should we split on number of siblings first?\n",
    "These are the kind of questions that are asked.\n",
    "If we split on age, what should we split at?\n",
    "9 1/2?\n",
    "10, 11, 12, 20, 40, 60?\n",
    "There are many choices.\n",
    "Decision tree tries to figure out\n",
    "what the splitting points are and which particular feature\n",
    "it should split on first.\n",
    "\n",
    "And the other thing in a decision tree\n",
    "that you to watch out for is that though the tree is\n",
    "__complete__, that means _every case can get processed through this_,\n",
    "it might actually be better on some branches\n",
    "and worse on other branches.\n",
    "\n",
    "So you might want to just take some branches.\n",
    "And the best example of that is if you have a trading strategy,\n",
    "right?\n",
    "In a trading strategy, what you're trying to do\n",
    "is you're trying to predict whether you're\n",
    "going to make money or not.\n",
    "So what you might have is a whole bunch\n",
    "of technical features and fundamental features\n",
    "about a stock, and you can build out a huge decision tree.\n",
    "What happens if the price-earnings ratio\n",
    "is greater than something?\n",
    "What happens if the price book is less than something,\n",
    "the 10-day previous return is greater\n",
    "than something, et cetera?\n",
    "And you might find that only one set, that\n",
    "is one branch of this whole thing, actually makes sense.\n",
    "And that branch is good enough that you can buy and make money\n",
    "on that.\n",
    "\n",
    "That doesn't mean that--\n",
    "the entire tree doesn't mean that you\n",
    "can use every result of it.\n",
    "Maybe you're missing out on many good trades,\n",
    "but what you've done is, with some high confidence,\n",
    "you identified some good trades, and so you go with that.\n",
    "\n",
    "So the __nice thing, compared to regression, with a decision\n",
    "tree is that you can focus on small parts\n",
    "of your final outcome and ignore the other parts__ and say,\n",
    "whether they work well or not, I don't care.\n",
    "That's really the nice thing about a decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Stopping and Pruning Rules</h3>\n",
    "<li>A minimum count of observations in each leaf node\n",
    "<li>A maximum tree <b>depth</b>\n",
    "<li>A maximum <b>complexity</b> (the number of splits)\n",
    "<li>Using all three, you won't necessarily have a balanced tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting wine quality using a decision tree\n",
    "### Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.319637</td>\n",
       "      <td>0.527821</td>\n",
       "      <td>0.270976</td>\n",
       "      <td>2.538806</td>\n",
       "      <td>0.087467</td>\n",
       "      <td>15.874922</td>\n",
       "      <td>46.467792</td>\n",
       "      <td>0.996747</td>\n",
       "      <td>3.311113</td>\n",
       "      <td>0.658149</td>\n",
       "      <td>10.422983</td>\n",
       "      <td>5.636023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.741096</td>\n",
       "      <td>0.179060</td>\n",
       "      <td>0.194801</td>\n",
       "      <td>1.409928</td>\n",
       "      <td>0.047065</td>\n",
       "      <td>10.460157</td>\n",
       "      <td>32.895324</td>\n",
       "      <td>0.001887</td>\n",
       "      <td>0.154386</td>\n",
       "      <td>0.169507</td>\n",
       "      <td>1.065668</td>\n",
       "      <td>0.807569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.600000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.012000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.990070</td>\n",
       "      <td>2.740000</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>8.400000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7.100000</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.995600</td>\n",
       "      <td>3.210000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.900000</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>0.079000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>0.996750</td>\n",
       "      <td>3.310000</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>10.200000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9.200000</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>0.997835</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>11.100000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>15.900000</td>\n",
       "      <td>1.580000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>0.611000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>289.000000</td>\n",
       "      <td>1.003690</td>\n",
       "      <td>4.010000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>14.900000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
       "count    1599.000000       1599.000000  1599.000000     1599.000000   \n",
       "mean        8.319637          0.527821     0.270976        2.538806   \n",
       "std         1.741096          0.179060     0.194801        1.409928   \n",
       "min         4.600000          0.120000     0.000000        0.900000   \n",
       "25%         7.100000          0.390000     0.090000        1.900000   \n",
       "50%         7.900000          0.520000     0.260000        2.200000   \n",
       "75%         9.200000          0.640000     0.420000        2.600000   \n",
       "max        15.900000          1.580000     1.000000       15.500000   \n",
       "\n",
       "         chlorides  free sulfur dioxide  total sulfur dioxide      density  \\\n",
       "count  1599.000000          1599.000000           1599.000000  1599.000000   \n",
       "mean      0.087467            15.874922             46.467792     0.996747   \n",
       "std       0.047065            10.460157             32.895324     0.001887   \n",
       "min       0.012000             1.000000              6.000000     0.990070   \n",
       "25%       0.070000             7.000000             22.000000     0.995600   \n",
       "50%       0.079000            14.000000             38.000000     0.996750   \n",
       "75%       0.090000            21.000000             62.000000     0.997835   \n",
       "max       0.611000            72.000000            289.000000     1.003690   \n",
       "\n",
       "                pH    sulphates      alcohol      quality  \n",
       "count  1599.000000  1599.000000  1599.000000  1599.000000  \n",
       "mean      3.311113     0.658149    10.422983     5.636023  \n",
       "std       0.154386     0.169507     1.065668     0.807569  \n",
       "min       2.740000     0.330000     8.400000     3.000000  \n",
       "25%       3.210000     0.550000     9.500000     5.000000  \n",
       "50%       3.310000     0.620000    10.200000     6.000000  \n",
       "75%       3.400000     0.730000    11.100000     6.000000  \n",
       "max       4.010000     2.000000    14.900000     8.000000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "import pandas as pd\n",
    "w_df = pd.read_csv(url,header=0,sep=';')\n",
    "w_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.4              0.70         0.00             1.9      0.076   \n",
       "1            7.8              0.88         0.00             2.6      0.098   \n",
       "2            7.8              0.76         0.04             2.3      0.092   \n",
       "3           11.2              0.28         0.56             1.9      0.075   \n",
       "4            7.4              0.70         0.00             1.9      0.076   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
       "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
       "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      9.4        5  \n",
       "1      9.8        5  \n",
       "2      9.8        5  \n",
       "3      9.8        6  \n",
       "4      9.4        5  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1599, 12)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build train and test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>857</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      quality\n",
       "856         6\n",
       "745         6\n",
       "1048        6\n",
       "530         6\n",
       "84          6\n",
       "857         7\n",
       "356         5\n",
       "29          6\n",
       "134         6\n",
       "115         6"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train,test = train_test_split(w_df,test_size = 0.3)\n",
    "\n",
    "x_train = train.iloc[0:,0:11]\n",
    "y_train = train[['quality']]\n",
    "\n",
    "x_test = test.iloc[0:,0:11]\n",
    "y_test = test[['quality']]\n",
    "\n",
    "# Use all data for cross-validation\n",
    "x_data = w_df.iloc[0:,0:11]\n",
    "y_data = w_df[['quality']]\n",
    "\n",
    "y_test[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Classifiers vs Regressors</h3>\n",
    "<li>Decision tree regressors are used when the target variable is continuous and ordered (wine quality from 0 to 10)\n",
    "<li>Classifiers are used when the target variable is a set of unordered categories (rocks or mines)\n",
    "\n",
    "#### For wine quality, we need a regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=3, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "           splitter='best')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn import tree # to draw structure\n",
    "\n",
    "model = DecisionTreeRegressor(max_depth = 3)\n",
    "model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's estimate the prediction.\n",
    "The coefficient R^2 is defined as (1 - u/v), where \n",
    "\n",
    "- u is the residual sum of squares ((y_true - y_pred) ** 2).sum() \n",
    "- v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum()\n",
    "\n",
    "The __best possible score is 1.0__ and it can be __negative__ (because the model can be arbitrarily worse). \n",
    "\n",
    "A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training R-Square 0.340709769996\n",
      "Testing R-Square 0.293874521075\n"
     ]
    }
   ],
   "source": [
    "#Get the R-Square for the predicted vs actuals on the text sample\n",
    "print(\"Training R-Square\",model.score(x_train,y_train))\n",
    "print(\"Testing R-Square\",model.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we get fairly low R-squareds, 0.36 and 0.25.\n",
    "But again, note that though the R-squared is low,\n",
    "we don't worry too much about it, because like I\n",
    "said, maybe we want to focus only on identifying good wines.\n",
    "So let's say our goal is we don't care about messing up\n",
    "below 7.0 classification.\n",
    "But anything above 7.0, we wanted to call it a good wine.\n",
    "So if you can identify some good wines correctly,\n",
    "then that would be great.\n",
    "So R-squared is a good way to look at how good your model is.\n",
    "\n",
    "# View the tree\n",
    "<h3>Download and install <a href=\"http://www.graphviz.org/Download.php\">graphviz</a></h3>\n",
    "If you are having issues using Graphviz in Windows, then try the following steps:\n",
    "<ol>\n",
    "<li>1. Install Graphviz \n",
    "<li>2. After installing graphviz, add it to the Computer's Path. \n",
    "<ul>\n",
    "<li>Go to PC properties \n",
    "<li> Click environment variables in the advanced settings section\n",
    "<li> Add C:\\Program Files (x86)\\Graphviz2.38\\bin\\ to the PATH and click Apply\n",
    "</ul>\n",
    "<li> Install Pydotplus. Note that you will always have to install pydot after graphviz as Pydot is Graphviz's dot language and needs Graphviz for reference. \n",
    "</ol>\n",
    "<h3>Install pydotplus (using pip): Install graphviz before you install pydotplus!</h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pydotplus \n",
    "feature_names = [key for key in w_df]\n",
    "dot_data = tree.export_graphviz(model.tree_, out_file=None,feature_names=feature_names) \n",
    "graph = pydotplus.graph_from_dot_data(dot_data) \n",
    "graph.write_pdf(\"wines.pdf\") \n",
    "#The tree will be saved to wines.pdf in your current directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h2>Decision trees are Entropy minimizers</h2>\n",
    "\n",
    "So what exactly do decision trees do?\n",
    "Well, decision trees try to __minimize entropy__.\n",
    "What is entropy?\n",
    "<li><b>Entropy</b>: a measure of uncertainty in the data<p>\n",
    "Entropy is the degree of variance inside the data.\n",
    "So what we're going to do is we're going to take our data\n",
    "and split it up into sets where the variance is lower.\n",
    "With the combined variance, you add up the variances, you know.\n",
    "\n",
    "You don't physically add them up, but you know what I mean.\n",
    "\n",
    "When you take the variance of the two sets and the combined\n",
    "variance is lower than the variance of the combined set.\n",
    "\n",
    "So the variance, adding up of the variance of the two subsets\n",
    "is lower than the variance of the overall data.\n",
    "You might end up with a higher variance in one of the subsets\n",
    "than you have in the overall data,\n",
    "but the combined variance has to be lower.\n",
    "<li><b>Entropy minimization</b>: decision tree algorithms seek to partition the data on features in the way that total entropy is minimized\n",
    "\n",
    "So if you take a box of 100 marbles, which\n",
    "are blue and red, and we can split them\n",
    "up so, with the mix, then your probability when\n",
    "you draw a marble, the probability\n",
    "that it's going to be blue is 50%.\n",
    "But if you can split them up into two groups\n",
    "where one group has all 50 blue and the other has\n",
    "all 50 red, then the combined probability, you know--\n",
    "because you have these two groups\n",
    "and you know for certain that one group has all blues\n",
    "and the other has all reds-- then the probability of picking\n",
    "a blue or a red marble is 1, right?\n",
    "Because you split it up.\n",
    "\n",
    "And that's what a decision tree is trying to do.\n",
    "It's trying to find some rule that\n",
    "splits the marble by color.\n",
    "\n",
    "So for example, if the blue marbles\n",
    "happen to be bigger than the red marbles,\n",
    "right, let's say the blue marbles are 1 inch radius--\n",
    "that's pretty big for a marble--\n",
    "1 inch diameter, and the red marbles\n",
    "are 1/2 an inch diameter, so you split on marble size\n",
    "and you get two groups that are now all blue or all red.\n",
    "But you split on some other variable.\n",
    "You split it on a feature of the marble that is not the color.\n",
    "\n",
    "The _color_ is a __dependent__ variable\n",
    "and the __feature__ is the _size_.\n",
    "And that's how the decision tree model really works.\n",
    "So it tries to minimize the combined entropy of the model.\n",
    "The regression tree, what it's doing is \n",
    "it __runs regressions for each of the independent variables\n",
    "on the dependent variable__.\n",
    "\n",
    "It picks the variable that has the most _explanatory power_\n",
    "and splits it at several points.\n",
    "So it'll take the variable and say, all right,\n",
    "fixed acidity has the most explanatory power.\n",
    "It splits that on several points and decides\n",
    "which point gives you the best discrimination between the two\n",
    "halves.\n",
    "\n",
    "In the regression tree, that means\n",
    "that it looks at the __mean square error__ in the two\n",
    "halves of each split.\n",
    "\n",
    "_And then finally, it picks the split point that gives you\n",
    "the __lowest__ mean square error_.\n",
    "\n",
    "So in our tree here, what it was doing was,\n",
    "it was looking at the impurity, which\n",
    "is this case with the mean square error,\n",
    "since it's a regression tree.\n",
    "<img src='wines_pdf.png'>\n",
    "And we're dropping the mean square error from 0.6601\n",
    "all the way down to 0.3951 over here, or 0.3716 over here.\n",
    "So this is what's going on over there with this thing here.\n",
    "And so it's picking the split points and works that stuff\n",
    "there.\n",
    "So that's how that works.\n",
    "\n",
    "And the larger sample set we get,\n",
    "the more reasonably we can expect\n",
    "that to reflect the actual entropy.\n",
    "So if we get a larger sample set, then presumably--\n",
    "and you know, enough variation captured in that sample set--\n",
    "then presumably that would work better\n",
    "in the outside, unseen cases.\n",
    "And if we don't get a larger sample set,\n",
    "then it won't work as well in unseen cases.\n",
    "That's the idea there.\n",
    "\n",
    "So if you're working with smaller sample sets,\n",
    "then we can use a technique called __cross-validation__.\n",
    "\n",
    "So the idea here is very straightforward.\n",
    "We have a sample in our training and test combined.\n",
    "And we divide it up into a training and a testing.\n",
    "We train on the training and we tested on the testing.\n",
    "And that gives us an estimate of how good our model is.\n",
    "\n",
    "However, if we pick a different training set and a different testing\n",
    "set, we divide up our original data into different training\n",
    "and testing set, we may get a very dramatically\n",
    "different result. If that happens,\n",
    "then our model is not going to be robust because it's\n",
    "really dependent upon how we split the data into training\n",
    "and testing.\n",
    "And that's not so good.\n",
    "\n",
    "So we can use a technique called __cross-validation__ to see\n",
    "how robust our model is.\n",
    "So what we do in cross-validation is,\n",
    "you take the entire data set and __split it up__\n",
    "into __k smaller sets__.\n",
    "And then you train the data on k minus 1 of those sets.\n",
    "So if you have like 100 data items,\n",
    "you can split it up into 10 smaller\n",
    "sets of 10 data items each.\n",
    "You run a training tree on 90--\n",
    "on the first 9, that is 90 of them--\n",
    "and test it on the last 10.\n",
    "And then use the results from your testing and store that\n",
    "result. \n",
    "\n",
    "Then you take another 9, which is not the original 9--\n",
    "maybe like 1, 2, 3, 4, 5, 6, 7, and 10--\n",
    "and test it on 9.\n",
    "And then take 1, 2, 3, 4, 5, 6, 7, and then 9 and 10,\n",
    "and test it on 8, et cetera.\n",
    "\n",
    "So these are called __folds__.\n",
    "And you run it on different subsets\n",
    "of the data, _different subset of the folds_,\n",
    "and __measure the mean square error__.\n",
    "The idea being, in a regression tree--\n",
    "or classification tree-- the idea being\n",
    "that on each time that you run it,\n",
    "you __get a mean square error that's fairly similar to what\n",
    "you'll be getting with different subsets__,\n",
    "then your final results are likely to be\n",
    "more robust because they're __not dependent on where\n",
    "you split your training and testing sample__.\n",
    "That's the idea here.\n",
    "\n",
    "_You take the average of all test performance metrics_.\n",
    "\n",
    "<h3>Regression trees</h3>\n",
    "<li>Run regressions for each X to the dependent variable\n",
    "<li>Pick the variable with the most explanatory power and split it at several points\n",
    "<li>Calculate the Mean Square Error of each of the two halves for each split\n",
    "<li>Pick the split point that gives the lowest mse (combined)\n",
    "\n",
    "<h2>Cross validation</h2>\n",
    "<li>Split the training set into k smaller sets (aka folds)\n",
    "<li>Train the data on k-1 folds\n",
    "<li>Validate the results on fold k\n",
    "<li>Repeat this holding out each of the k folds in turn\n",
    "<li>Report the average of all tests as the performance metric\n",
    "<li>http://scikit-learn.org/stable/modules/cross_validation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So when we do the folding, we want\n",
    "to shuffle our data first and then do the picking,\n",
    "rather than just pick randomly, just\n",
    "pick them linearly from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crossvalidation = KFold(n_splits=5,shuffle=True,random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So I run the cross-validation here--\n",
    "\n",
    "And now what I want to do is, I have 5 folds\n",
    "and I want to create many, many trees.\n",
    "So I'm going to create trees that range \n",
    "in depth from 1 to 10.\n",
    "So I don't know, for example, whether a depth 3\n",
    "tree or a depth 5 tree or a depth 10 tress is better.\n",
    "I don't know, right?\n",
    "So what I want to do, I want to say, all right,\n",
    "run them all from 1 to 10.\n",
    "\n",
    "I run the decision, call the decision tree regressor,\n",
    "and then I fit the data, making sure that the max\n",
    "depth is controlled over there.\n",
    "And then I compute the mean of the cross-validation score\n",
    "for those trees over there and store that in a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth: 1 Accuracy: -0.548\n",
      "Depth: 2 Accuracy: -0.512\n",
      "Depth: 3 Accuracy: -0.482\n",
      "Depth: 4 Accuracy: -0.482\n",
      "Depth: 5 Accuracy: -0.480\n",
      "Depth: 6 Accuracy: -0.493\n",
      "Depth: 7 Accuracy: -0.535\n",
      "Depth: 8 Accuracy: -0.573\n",
      "Depth: 9 Accuracy: -0.600\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "import numpy as np\n",
    "for depth in range(1,10):\n",
    "    model = tree.DecisionTreeRegressor(\n",
    "    max_depth=depth,random_state=0)\n",
    "    if model.fit(x_data,y_data).tree_.max_depth < depth:\n",
    "        break\n",
    "    score = np.mean(cross_val_score(model,\n",
    "                                    x_data,\n",
    "                                    y_data,\n",
    "                                    scoring='neg_mean_squared_error', \n",
    "                                    cv=crossvalidation, \n",
    "                                    n_jobs=1))\n",
    "    print ('Depth: %i Accuracy: %.3f' % (depth,score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So I get these nine depth values.\n",
    "And the idea is to see whether these depth values,\n",
    "or these different values that you're\n",
    "getting from the cross-validation tests\n",
    "are __reasonably similar__.\n",
    "\n",
    "In this case, we can see they are reasonably similar.\n",
    "If they are reasonably similar, then that\n",
    "means that we are a model that is __likely to be more robust__.\n",
    "So what is the tree in this case?\n",
    "In this case, we don't really have a tree\n",
    "because we have many, many trees.\n",
    "And what we're really doing is, we are not generating a tree.\n",
    "We're generating many trees.\n",
    "But we want to get an estimate for the average error\n",
    "of the model.\n",
    "That's what we're trying to do.\n",
    "\n",
    "<h3>Purpose of cross-validation</h3>\n",
    "<li>Not to generate a tree (it generates many trees!)\n",
    "<li>But to provide an estimate of the average error of the model - _the best will be depth=5_\n",
    "<li>Roughly, the idea is to see how the model performance varies with different training sets\n",
    "\n",
    "And then once we do that, we can pick a depth.\n",
    "And we have our model.\n",
    "And now we want to actually generate the tree itself.\n",
    "So now we go back and no longer use a training and testing\n",
    "sample.\n",
    "Because we've run the thing on each individual folds\n",
    "and we are saying we don't really\n",
    "want to use a training and testing here\n",
    "because we don't want our tree to be subject to our decision\n",
    "on where to, which ones to include in the training, which\n",
    "cases to include in the training and which\n",
    "to include in the testing.\n",
    "So we run the data on the entire tree.\n",
    "<li>To generate the tree, use the entire training set as before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Classification trees</h1>\n",
    "\n",
    "Let's look at classification trees.\n",
    "And so, for example, in a mines and rock example,\n",
    "we want to figure out whether something is a mine or a rock.\n",
    "\n",
    "And like I said before, if we can figure out\n",
    "the mines very well or the rocks very well, we may be all right.\n",
    "\n",
    "We don't really want to be accurate on both\n",
    "of them that's the point there.\n",
    "So what are we going to do is, we, in __classification tree__,\n",
    "we're __looking at the misclassification cost__, which\n",
    "is the measure called __GINI__cost function that does that.\n",
    "<h2>Classification trees are used when dealing with categorical dependent variables</h2>\n",
    "<li>Pick a variable and a split point so that the misclassification cost is the lowest.\n",
    "\n",
    "<h3>Rocks and mines data set</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>208.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.029164</td>\n",
       "      <td>0.038437</td>\n",
       "      <td>0.043832</td>\n",
       "      <td>0.053892</td>\n",
       "      <td>0.075202</td>\n",
       "      <td>0.104570</td>\n",
       "      <td>0.121747</td>\n",
       "      <td>0.134799</td>\n",
       "      <td>0.178003</td>\n",
       "      <td>0.208259</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013420</td>\n",
       "      <td>0.010709</td>\n",
       "      <td>0.010941</td>\n",
       "      <td>0.009290</td>\n",
       "      <td>0.008222</td>\n",
       "      <td>0.007820</td>\n",
       "      <td>0.007949</td>\n",
       "      <td>0.007941</td>\n",
       "      <td>0.006507</td>\n",
       "      <td>0.533654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.022991</td>\n",
       "      <td>0.032960</td>\n",
       "      <td>0.038428</td>\n",
       "      <td>0.046528</td>\n",
       "      <td>0.055552</td>\n",
       "      <td>0.059105</td>\n",
       "      <td>0.061788</td>\n",
       "      <td>0.085152</td>\n",
       "      <td>0.118387</td>\n",
       "      <td>0.134416</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009634</td>\n",
       "      <td>0.007060</td>\n",
       "      <td>0.007301</td>\n",
       "      <td>0.007088</td>\n",
       "      <td>0.005736</td>\n",
       "      <td>0.005785</td>\n",
       "      <td>0.006470</td>\n",
       "      <td>0.006181</td>\n",
       "      <td>0.005031</td>\n",
       "      <td>0.500070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>0.006700</td>\n",
       "      <td>0.010200</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>0.011300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.013350</td>\n",
       "      <td>0.016450</td>\n",
       "      <td>0.018950</td>\n",
       "      <td>0.024375</td>\n",
       "      <td>0.038050</td>\n",
       "      <td>0.067025</td>\n",
       "      <td>0.080900</td>\n",
       "      <td>0.080425</td>\n",
       "      <td>0.097025</td>\n",
       "      <td>0.111275</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007275</td>\n",
       "      <td>0.005075</td>\n",
       "      <td>0.005375</td>\n",
       "      <td>0.004150</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>0.003675</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.022800</td>\n",
       "      <td>0.030800</td>\n",
       "      <td>0.034300</td>\n",
       "      <td>0.044050</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.092150</td>\n",
       "      <td>0.106950</td>\n",
       "      <td>0.112100</td>\n",
       "      <td>0.152250</td>\n",
       "      <td>0.182400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011400</td>\n",
       "      <td>0.009550</td>\n",
       "      <td>0.009300</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>0.006850</td>\n",
       "      <td>0.005950</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>0.005300</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.035550</td>\n",
       "      <td>0.047950</td>\n",
       "      <td>0.057950</td>\n",
       "      <td>0.064500</td>\n",
       "      <td>0.100275</td>\n",
       "      <td>0.134125</td>\n",
       "      <td>0.154000</td>\n",
       "      <td>0.169600</td>\n",
       "      <td>0.233425</td>\n",
       "      <td>0.268700</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016725</td>\n",
       "      <td>0.014900</td>\n",
       "      <td>0.014500</td>\n",
       "      <td>0.012100</td>\n",
       "      <td>0.010575</td>\n",
       "      <td>0.010425</td>\n",
       "      <td>0.010350</td>\n",
       "      <td>0.010325</td>\n",
       "      <td>0.008525</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.137100</td>\n",
       "      <td>0.233900</td>\n",
       "      <td>0.305900</td>\n",
       "      <td>0.426400</td>\n",
       "      <td>0.401000</td>\n",
       "      <td>0.382300</td>\n",
       "      <td>0.372900</td>\n",
       "      <td>0.459000</td>\n",
       "      <td>0.682800</td>\n",
       "      <td>0.710600</td>\n",
       "      <td>...</td>\n",
       "      <td>0.070900</td>\n",
       "      <td>0.039000</td>\n",
       "      <td>0.035200</td>\n",
       "      <td>0.044700</td>\n",
       "      <td>0.039400</td>\n",
       "      <td>0.035500</td>\n",
       "      <td>0.044000</td>\n",
       "      <td>0.036400</td>\n",
       "      <td>0.043900</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0           1           2           3           4           5   \\\n",
       "count  208.000000  208.000000  208.000000  208.000000  208.000000  208.000000   \n",
       "mean     0.029164    0.038437    0.043832    0.053892    0.075202    0.104570   \n",
       "std      0.022991    0.032960    0.038428    0.046528    0.055552    0.059105   \n",
       "min      0.001500    0.000600    0.001500    0.005800    0.006700    0.010200   \n",
       "25%      0.013350    0.016450    0.018950    0.024375    0.038050    0.067025   \n",
       "50%      0.022800    0.030800    0.034300    0.044050    0.062500    0.092150   \n",
       "75%      0.035550    0.047950    0.057950    0.064500    0.100275    0.134125   \n",
       "max      0.137100    0.233900    0.305900    0.426400    0.401000    0.382300   \n",
       "\n",
       "               6           7           8           9      ...              51  \\\n",
       "count  208.000000  208.000000  208.000000  208.000000     ...      208.000000   \n",
       "mean     0.121747    0.134799    0.178003    0.208259     ...        0.013420   \n",
       "std      0.061788    0.085152    0.118387    0.134416     ...        0.009634   \n",
       "min      0.003300    0.005500    0.007500    0.011300     ...        0.000800   \n",
       "25%      0.080900    0.080425    0.097025    0.111275     ...        0.007275   \n",
       "50%      0.106950    0.112100    0.152250    0.182400     ...        0.011400   \n",
       "75%      0.154000    0.169600    0.233425    0.268700     ...        0.016725   \n",
       "max      0.372900    0.459000    0.682800    0.710600     ...        0.070900   \n",
       "\n",
       "               52          53          54          55          56          57  \\\n",
       "count  208.000000  208.000000  208.000000  208.000000  208.000000  208.000000   \n",
       "mean     0.010709    0.010941    0.009290    0.008222    0.007820    0.007949   \n",
       "std      0.007060    0.007301    0.007088    0.005736    0.005785    0.006470   \n",
       "min      0.000500    0.001000    0.000600    0.000400    0.000300    0.000300   \n",
       "25%      0.005075    0.005375    0.004150    0.004400    0.003700    0.003600   \n",
       "50%      0.009550    0.009300    0.007500    0.006850    0.005950    0.005800   \n",
       "75%      0.014900    0.014500    0.012100    0.010575    0.010425    0.010350   \n",
       "max      0.039000    0.035200    0.044700    0.039400    0.035500    0.044000   \n",
       "\n",
       "               58          59          60  \n",
       "count  208.000000  208.000000  208.000000  \n",
       "mean     0.007941    0.006507    0.533654  \n",
       "std      0.006181    0.005031    0.500070  \n",
       "min      0.000100    0.000600    0.000000  \n",
       "25%      0.003675    0.003100    0.000000  \n",
       "50%      0.006400    0.005300    1.000000  \n",
       "75%      0.010325    0.008525    1.000000  \n",
       "max      0.036400    0.043900    1.000000  \n",
       "\n",
       "[8 rows x 61 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "url=\"https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data\"\n",
    "df = pd.read_csv(url,header=None)\n",
    "df[60]=np.where(df[60]=='R',0,1)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>145 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     60\n",
       "166   1\n",
       "128   1\n",
       "181   1\n",
       "177   1\n",
       "82    0\n",
       "145   1\n",
       "136   1\n",
       "96    0\n",
       "132   1\n",
       "118   1\n",
       "65    0\n",
       "57    0\n",
       "120   1\n",
       "193   1\n",
       "206   1\n",
       "12    0\n",
       "64    0\n",
       "59    0\n",
       "161   1\n",
       "157   1\n",
       "197   1\n",
       "38    0\n",
       "140   1\n",
       "83    0\n",
       "139   1\n",
       "178   1\n",
       "124   1\n",
       "40    0\n",
       "13    0\n",
       "52    0\n",
       "..   ..\n",
       "176   1\n",
       "99    1\n",
       "187   1\n",
       "91    0\n",
       "53    0\n",
       "46    0\n",
       "93    0\n",
       "71    0\n",
       "101   1\n",
       "125   1\n",
       "159   1\n",
       "202   1\n",
       "190   1\n",
       "25    0\n",
       "135   1\n",
       "51    0\n",
       "154   1\n",
       "121   1\n",
       "130   1\n",
       "28    0\n",
       "42    0\n",
       "33    0\n",
       "109   1\n",
       "34    0\n",
       "204   1\n",
       "165   1\n",
       "80    0\n",
       "60    0\n",
       "189   1\n",
       "78    0\n",
       "\n",
       "[145 rows x 1 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train,test = train_test_split(df,test_size=0.3)\n",
    "x_train = train.iloc[0:,0:60]\n",
    "y_train = train[[60]]\n",
    "x_test = test.iloc[0:,0:60]\n",
    "y_test = test[[60]]\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
